---
title: "Evaluation criteria - IST-3 Data"
output: 
  rmdformats::downcute
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r,include=FALSE}

library(synthpop)
library(caret)
library(stringdist)
library(dplyr)
library(arf)
library(ranger)
library(purrr) 
library(caret)      
library(randomForest) 
library(knitr)
library(FNN) 
library(ggplot2)
library(tidyr)
library(data.table)
library(cluster)
library(keras)
library(class)
library(pROC)

```

```{r, include=FALSE}

getwd()


source("functions.R")


data_small <- readRDS("Raw Data/data_small.Rds")
real_data <- data_small

real_train <- read.csv("Raw Data/real_train.csv")
real_test <- read.csv("Raw Data/real_holdout.csv")
real_holdout <- real_test


# Alpha precision, beta recall, authenticity
more_criteria_IST_3_precision_recall_authenticity <- read.csv("evaluation_more_criteria/synthetic_data_evaluation_precision_recall_authenticity.csv")


# C2ST 
more_criteria_IST_3_c2st <- read.csv("evaluation_more_criteria/synthetic_data_evaluation_c2st_avg.csv")
more_criteria_IST_3_mean_c2st <- read.csv("evaluation_more_criteria/synthetic_data_evaluation_mean_c2st_avg.csv")


# ML utility
more_criteria_IST_3_ml_utility <- read.csv("evaluation_more_criteria/synthetic_data_ML_utility.csv")
more_criteria_IST_3_mean_ml_utility <- read.csv("evaluation_more_criteria/synthetic_data_mean_ML_utility.csv")


# SIR
more_criteria_IST_3_sir <- read.csv("evaluation_more_criteria/synthetic_data_identical_records.csv")
more_criteria_IST_3_mean_sir <- read.csv("evaluation_more_criteria/synthetic_data_mean_identical_records.csv")


# DCR
more_criteria_IST_3_dcr <- read.csv("evaluation_more_criteria/synthetic_data_dcr.csv")
more_criteria_IST_3_mean_dcr <- read.csv("evaluation_more_criteria/synthetic_data_dcr_mean.csv")


# NNDR
more_criteria_IST_3_nndr <- read.csv("evaluation_more_criteria/synthetic_data_nndr.csv")
more_criteria_IST_3_mean_nndr <- read.csv("evaluation_more_criteria/synthetic_data_mean_nndr.csv")


```


```{r}

str(data_small)

summary(data_small)

```



# Resemblance

## Alpha precision, beta recall, authenticity

### m = 5

```{r}

more_criteria_IST_3_precision_recall_authenticity_m5 <- more_criteria_IST_3_precision_recall_authenticity %>% filter(M == 5)

# Create the table using kable
kable(more_criteria_IST_3_precision_recall_authenticity_m5, format = "markdown", caption = "Results Table")

```

### m = 10

```{r}

more_criteria_IST_3_precision_recall_authenticity_m10 <- more_criteria_IST_3_precision_recall_authenticity %>% filter(M == 10)

# Create the table using kable
kable(more_criteria_IST_3_precision_recall_authenticity_m10, format = "markdown", caption = "Results Table")

```


### m = 50

```{r}

more_criteria_IST_3_precision_recall_authenticity_m50 <- more_criteria_IST_3_precision_recall_authenticity %>% filter(M == 50)

# Create the table using kable
kable(more_criteria_IST_3_precision_recall_authenticity_m50, format = "markdown", caption = "Results Table")

```

## Classifier two-sample test (C2ST)

Note: I have specified the limits of the y axis from 0.4 to 1. This is probably not the best option as some boxplots get very thin because of the big range but a consistent range for all plots would be nice.


### KNN 


#### Accuracy

```{r}

more_criteria_IST_3_c2st_knn <- more_criteria_IST_3_c2st %>% filter(Classifier == "KNN")

# Boxplot erstellen
plot_IST_3_c2st <-ggplot(more_criteria_IST_3_c2st_knn, aes(x = factor(M), y = C2ST_Accuracy, fill = Method)) +
  geom_boxplot() +
  labs(title = "Comparison of C2ST Accuracy by Method and Number of Synthetic Datasets",
       x = "M",
       y = "Mean Accuracy") +
  theme_minimal() + 
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(limits = c(0.4, 1), breaks = seq(0.4, 1, by = 0.1))  
plot_IST_3_c2st


```

Interpretation in general:

* Accuracy ~ 50% -> p-value high (> 0.05) -> $H_0$ is not rejected -> same distributions
* Accuracy ~ 55%-65% -> p-value moderate (= 0.05) -> weak evidence of difference
* Accuracy ~ 65%-80% -> p-value low (< 0.05) -> $H_0$ is rejected -> evidence that distributions are different
* Accuracy ~ 80%-100% -> p-value very low (< 0.001) -> $H_0$ is rejected -> samples are from distinct distributions


Key observations:

* The accuracy is for all four methods quite high (> 75%)
* The p-values are very low
* Resemblance is not high but the best for Synthpop and ARF

#### AUC

```{r}

# Boxplot erstellen
plot_IST_3_c2st <-ggplot(more_criteria_IST_3_c2st_knn, aes(x = factor(M), y = AUC, fill = Method)) +
  geom_boxplot() +
  labs(title = "Comparison of C2ST AUC by Method and Number of Synthetic Datasets",
       x = "M",
       y = "Mean AUC") +
  theme_minimal() + 
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(limits = c(0.4, 1), breaks = seq(0.4, 1, by = 0.1))  
plot_IST_3_c2st




```


### RF

#### Accuracy

```{r}

more_criteria_IST_3_c2st_rf <- more_criteria_IST_3_c2st %>% filter(Classifier == "RF")

# Boxplot erstellen
plot_IST_3_c2st <- ggplot(more_criteria_IST_3_c2st_rf, aes(x = factor(M), y = C2ST_Accuracy, fill = Method)) +
  geom_boxplot() +
  labs(title = "Comparison of C2ST Accuracy by Method and Number of Synthetic Datasets",
       x = "M",
       y = "Mean Accuracy") +
  theme_minimal() + 
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(limits = c(0.4, 1), breaks = seq(0.4, 1, by = 0.1))  
plot_IST_3_c2st




```

#### AUC

```{r}

more_criteria_IST_3_c2st_rf <- more_criteria_IST_3_c2st %>% filter(Classifier == "RF")

filtered_data <- more_criteria_IST_3_c2st_rf %>%
  filter(M == 5) %>%
  mutate(Method = fct_recode(Method,
                             ARF = "arf",
                             CTGAN = "ctgan",
                             PrivBayes = "privbayes",
                             Synthpop = "synthpop",
                             TABSYN = "tabsyn",
                             TVAE = "tvae"))

plot_m5 <- ggplot(filtered_data, aes(x = Method, y = AUC, fill = Method)) +
  geom_boxplot() +
  labs(x = "Method", y = "AUC") +
  theme_bw() +
  scale_fill_brewer(palette = "Set2") +
  theme(legend.position = "none")

plot_m5



ggsave(
  filename = "c2st_rf_auc_m5.pdf",
  plot = plot_m5,
  device = "pdf",
  path = "evaluation_utility/global_utility/",
  width = 8,
  height = 6
)



```

```{r}

# Boxplot erstellen
plot_IST_3_c2st <-ggplot(more_criteria_IST_3_c2st_rf, aes(x = factor(M), y = AUC, fill = Method)) +
  geom_boxplot() +
  labs(title = "Comparison of C2ST AUC by Method and Number of Synthetic Datasets",
       x = "M",
       y = "Mean AUC") +
  theme_minimal() + 
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(limits = c(0.4, 1), breaks = seq(0.4, 1, by = 0.1)) 
plot_IST_3_c2st






```


### NN

#### Accuracy

```{r}

more_criteria_IST_3_c2st_nn <- more_criteria_IST_3_c2st %>% filter(Classifier == "NN")

# Boxplot erstellen
plot_IST_3_c2st <-ggplot(more_criteria_IST_3_c2st_nn, aes(x = factor(M), y = C2ST_Accuracy, fill = Method)) +
  geom_boxplot() +
  labs(title = "Comparison of C2ST Accuracy by Method and Number of Synthetic Datasets",
       x = "M",
       y = "Mean Accuracy") +
  theme_minimal() + 
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(limits = c(0.4, 1), breaks = seq(0.4, 1, by = 0.1)) 
plot_IST_3_c2st




```

#### AUC

```{r}

# Boxplot erstellen
plot_IST_3_c2st <-ggplot(more_criteria_IST_3_c2st_nn, aes(x = factor(M), y = AUC, fill = Method)) +
  geom_boxplot() +
  labs(title = "Comparison of C2ST AUC by Method and Number of Synthetic Datasets",
       x = "M",
       y = "Mean AUC") +
  theme_minimal() + 
  scale_fill_brewer(palette = "Set2") +
  scale_y_continuous(limits = c(0.4, 1), breaks = seq(0.4, 1, by = 0.1)) 
plot_IST_3_c2st




```




# Utility

## Statistical Utility

Note: The colors still have to be changed to be consistent with the other plots.

```{r, echo=FALSE, out.width="100%", out.height="1000px"}

knitr::include_graphics("evaluation_utility/outcome_specific_utility/regression_coeff_plot_combined_m5.pdf")

```

## ML Utility

Es wird beurteilt, wie gut Modelle, die mit synthetischen Daten trainiert wurden, auf echten Daten performen. Bei Klassifikationsaufgaben werden Accuracy und/oder F1-Score betrachtet, während bei Regressionsaufgaben der R²-Wert (Bestimmtheitsmaß) betrachtet wird. 

```{r}
# Boxplot erstellen - R^2
plot_IST_3_ml_utility_r_squared <- ggplot(more_criteria_IST_3_ml_utility, aes(x = factor(M), y = Accuracy, fill = Method)) +
  geom_boxplot() +
  labs(x = "M",
       y = "Mean Accuracy") +
  theme_minimal() + 
  scale_fill_brewer(palette = "Set2") 
plot_IST_3_ml_utility_r_squared


```

```{r}

filtered_data <- more_criteria_IST_3_ml_utility %>%
  filter(M == 5) %>%
  mutate(Method = fct_recode(Method,
                             ARF = "arf",
                             CTGAN = "ctgan",
                             PrivBayes = "privbayes",
                             Synthpop = "synthpop",
                             TABSYN = "tabsyn",
                             TVAE = "tvae"))

plot_m5 <- ggplot(filtered_data, aes(x = Method, y = Accuracy, fill = Method)) +
  geom_boxplot() +
  labs(x = "Method", y = "Accuracy") +
  theme_bw() +
  scale_fill_brewer(palette = "Set2") +
  theme(legend.position = "none")

plot_m5


ggsave(
  filename = "ml_utility_m5.pdf",
  plot = plot_m5,
  device = "pdf",
  path = "evaluation_utility/outcome_specific_utility/",
  width = 8,
  height = 6
)

```


# Privacy

## Share of identical records (SIR)


```{r}

# Die Farben müssen noch angepasst werden, da ja noch TABSYN dazugekommen ist

plot_IST_3_sir <- ggplot(more_criteria_IST_3_mean_sir, aes(x = factor(M), y = Mean_IMS_Train_Syn, fill = Method)) +
  geom_bar(stat = "identity", position = "dodge", alpha = 0.8) +
  geom_hline(yintercept = mean(more_criteria_IST_3_mean_sir$Mean_IMS_Train_Test),
             linetype = "dotted", color = "black", size = 1) +
  theme_minimal() +
  labs(
    x = "Number of Synthetic Datasets",
    y = "Mean Share of Identical Records (in %)"
  ) +
  scale_fill_manual(values = c("#66C2A5", "#FC8D62", "#8DA0CB", "#E78AC3", "#a6d854", "#FFD92F")) + 
  theme(legend.position = "right")
plot_IST_3_sir

```

Key observations

* ARF, CTGAN, PrivBayes and TVAE have no identical records at all (SIR = 0)
* Synthpop has SIR, but not too much (~ 0.06%)



## Distance to closest records (DCR)

```{r}

# Boxplot erstellen
plot_IST_3_dcr <-ggplot(more_criteria_IST_3_dcr, aes(x = factor(M), y = DCR, fill = Method)) +
  geom_boxplot() +
  labs(title = "Comparison of the DCR by Method and Number of Synthetic Datasets",
       x = "M",
       y = "Mean DCR") +
  theme_minimal() + 
  scale_fill_brewer(palette = "Set2") 
plot_IST_3_dcr


```

In general: The distances between synthetic data and training data should not be systematically smaller than between training and holdout data.

Interpretation:

* The DCR values are small negative numbers.
* Since DCR is calculated as the difference between the minimum distances to the training and holdout sets, a negative value suggests that, on average, synthetic records are slightly closer to the holdout set than to the training set.
* This could indicate that the synthetic data captures the distribution well and does not overfit to the training data.
* More negative values may indicate potential overfitting to the training data.

* -> When the difference is negative, then the distances between synthetic data and training data are not systematically smaller than between training and holdout data
* -> good



## Nearest-neighbor distance ratio (NNDR)

```{r}

# Boxplot erstellen
plot_IST_3_nndr <- ggplot(more_criteria_IST_3_nndr, aes(x = factor(M), y = Mean_NNDR_Filtered, fill = Method)) +
  geom_boxplot() +
  labs(title = "Comparison of the NNDR by Method and Number of Synthetic Datasets",
       x = "M",
       y = "Mean NNDR") +
  theme_minimal() + 
  scale_fill_brewer(palette = "Set2") 
plot_IST_3_nndr

```


Interpretation in general:

* NNDR close to 1: The nearest and second-nearest neighbors are almost equally distant, meaning the synthetic data does not strongly resemble a specific real point. Suggests good privacy.
* NNDR close to 0: The nearest neighbor is much closer than the second-nearest, meaning the synthetic data point is highly similar to one real data point. This suggests higher privacy risk.
* Moderate NNDR: More balanced privacy and resemblance


